{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Section for Machine Learning A-Z\n",
    "Also, really good resource and introduction to ML on youtube: https://www.youtube.com/watch?v=hd1W4CyPX58&t=385s\n",
    "\n",
    "**Terminology:**\n",
    "\n",
    "**Supervised** - Trying to learn the relationship between the data (iris measurements) and the outcome (the species of iris). \n",
    "\n",
    "**Unsupervised learning** - only know the data/measurements, so we try to cluster the data into meaningful categories/groups.\n",
    "\n",
    "Each row is an **observation** (aka sample, example, instance, record).\n",
    "\n",
    "Each column is a **feature** (aka predictor, attribute, independent variable, input, regressor, covariate).\n",
    "\n",
    "Each value we are predicting is the **response** (aka target, outcome, label, dependent variable).\n",
    "\n",
    "**Classification** - is supervised learning in which the response is categorical (values are finite, unordered, email spam or ham? iris species classification, etc.).\n",
    "\n",
    "**Regression** - is supervised learning in which the response is ordered and continuous (price of a house, height of a person, etc.).\n",
    "\n",
    "Example calls:\n",
    "* iris.feature_names\n",
    "* iris.target\n",
    "* iris.target_names\n",
    "* iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for working with data in scikit-learn\n",
    "1. Features and response are **separate objects**\n",
    "2. Features and response should be **numeric**\n",
    "3. Features and response should be **NumPy arrays**\n",
    "4. Features and response should have **specific shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gaylonalfano/Documents/Python/Machine Learning A-Z/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the dataset but first need to set a working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      "Country      10 non-null object\n",
      "Age          9 non-null float64\n",
      "Salary       9 non-null float64\n",
      "Purchased    10 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 400.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important - Need to distingish matrix of features and dependent variables\n",
    "Independent Vars = Country, Age, Salary; Dependent Var = Purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix/array of independent vars\n",
    "X = dataset.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice slicing from np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 44.0, 72000.0], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 0] # countries\n",
    "X[0, 2] # 72000\n",
    "X[0:4, 0:2]\n",
    "X[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X  # returns np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dependent variable, Y\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-oriented Programming Basics\n",
    "A class is the model of something we want to build. For example, if we make a house construction plan that gathers the instructions on how to build a house, then this construction plan is the class.\n",
    "\n",
    "An object is an instance of the class. So if we take that same example of the house construction plan, then an object is simply a house. A house (the object) that was built by following the instructions of the construction plan (the class).\n",
    "And therefore there can be many objects of the same class, because we can build many houses from the construction plan.\n",
    "\n",
    "A method is a tool we can use on the object to complete a specific action. So in this same example, a tool can be to open the main door of the house if a guest is coming. A method can also be seen as a function that is applied onto the object, takes some inputs (that were defined in the class) and returns some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with missing data / NaN / nulls. Why do we have to .fit()?\n",
    "Here's a good explanation: The Imputer fills missing values with some statistics (e.g. mean, median, ...) of the data. To avoid data leakage during cross-validation, it computes the statistic on the train data during the fit, stores it and uses it on the test data, during the transform.\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "obj = Imputer(strategy='mean')\n",
    "\n",
    "obj.fit([[1, 2, 3], [2, 3, 4]])\n",
    "print(obj.statistics_)\n",
    "#array([ 1.5,  2.5,  3.5])\n",
    "\n",
    "X = obj.transform([[4, np.nan, 6], [5, 6, np.nan]])\n",
    "print(X)\n",
    "#array([[ 4. ,  2.5,  6. ],\n",
    "#[ 5. ,  6. ,  3.5]])\n",
    "You can do both steps in one if your train and test data are identical, using fit_transform.\n",
    "\n",
    "X = obj.fit_transform([[1, 2, np.nan], [2, 3, 4]])\n",
    "print(X)\n",
    "#array([[ 1. ,  2. ,  4. ],\n",
    "#[ 2. ,  3. ,  4. ]])\n",
    "This data leakage issue is important, since the data distribution may change from the training data to the testing data, and you don't want the information of the testing data to be already present during the fit.\n",
    "\n",
    "See the doc for more information about cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could remove the observations (not suggested)\n",
    "# Instead, should take the MEAN of the column and \n",
    "# use that to \"fill the value\" of null\n",
    "# Using sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values = \"NaN\", strategy = 'mean', axis = 0)\n",
    "\n",
    "# Now need to .fit() imputer object to the matrix X\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# After .fit(), need to set imputer to new value\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "\n",
    "# After \"fitting\" need to then replace the missing vals\n",
    "# with the mean of those columns using .transform()\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert to categorical ENCODED vals using sklearn.preprocessing LabelEncoder using .fit_transform() method\n",
    "1. First need to pass original array/series to the .fit_transform() method to return encoded array\n",
    "2. Then you need to store/save the encoded values. You can replace the original \"text\" values (\"France, Germany, Spain, etc.) with the encoded (1, 0, 2, 3, etc.) values. Not sure if you should just overwrite original column or add new column to matrix??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert Country and Purchased columns to\n",
    "# categories. Can I also use Pandas for this??\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step is you must create an object \n",
    "# of the LE class. #Then we use this object on \n",
    "#our categorical vars in the matrix\n",
    "labelencoder_X = LabelEncoder()\n",
    "\n",
    "# This line fits the obj to the first column of\n",
    "# our matrix X, but this time returns the ENCODED values\n",
    "# of the Countries columns. ENCODED seems to be the same \n",
    "# concept used in regression (like dummy vars?)\n",
    "\n",
    "# To apply the labelencoder object on our column\n",
    "# of countries, we need to call the .fit_transform() method\n",
    "# The method returns an encoded array of the original\n",
    "# array passed to the method (e.g., Country)\n",
    "labelencoder_X.fit_transform(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, need to save encoded values back into the\n",
    "# original matrix. Tutorial just replaces the values.\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "\n",
    "# What about trying to add a new column instead of\n",
    "# overwriting the original? With pandas DF, you just \n",
    "# add a new column df['Col'], but need to test with\n",
    "# np.matrix\n",
    "\n",
    "# X[:, 3] = np.arange(len(X)) - How to add column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns encoded. However, since it's numberic\n",
    "# we need to add dummy vars so the machine/pc doesn't\n",
    "# Think that 2 is greater than 0 (Germany > France)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns encoded. However, since it's numberic we need to add dummy vars so the machine learning equations don't think that 2 is greater than 0 (Germany > France). We need to create dummy vars uing the OneHotEncoder class\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can do the same thing in Pandas using get_dummies() method/function\n",
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn, there is another class, OneHotEncoder, that essentially\n",
    "# helps create the dummy var columns (1 or 0 for each categorical var)\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to pass index of categorical values (index 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "\n",
    "# Now object is ready so now need to fit to matrix X\n",
    "# Just need to pass (X) since the OHE knows that it's\n",
    "# looking at index[0] of whatever is passed to it w/ .fit_trans\n",
    "\n",
    "# After calling .fit_transform(X), need to pass .toarray()\n",
    "# to return an array output\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.40000000e+01, 7.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        2.70000000e+01, 4.80000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        3.00000000e+01, 5.40000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        3.80000000e+01, 6.10000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        4.00000000e+01, 6.37777778e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.50000000e+01, 5.80000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        3.87777778e+01, 5.20000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.80000000e+01, 7.90000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e+01, 8.30000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.70000000e+01, 6.70000000e+04]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: The difference between OneHotEncoder and LabelEncoder is that OHE is used to ensure that the machine learning algorithms do not attribute any order/ranking to the encoded categorical variables (1, 0, 2, etc.). So, if the variables are countries (France, China, USA, etc.), then there is no logical order or ranking, so we should use OHE. However, for T-shirt sizes S, M, L, there is an order/rank, so we should NOT use OHE, since we want the algorithm to know/understand the order between these categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To encode the dependent variable \"Purchased\" categorical values\n",
    "# (yes or no), only need to use the LabelEncoder since the algorithms\n",
    "# know that there is no order/ranking/difference between yes or no\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "# transforms y into a encoded variable vector/array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE for Section 2-15, change from sklearn.cross_validation import train_test_split TO from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into Train set and a Test set using sklearn.model_selection train_test_split library. Why?\n",
    "This is for the model/algorithm that is going to learn to do something on your dataset by understanding correlations on your dataset. This may learn too much as it cannot apply to an entirely new dataset. This is called overfitting your model.\n",
    "\n",
    "So we have to build two sets TRAIN and TEST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Build training and test sets. Use test_size = to set a portion of\n",
    "# your dataset to be assigned/used for your test set. Typically a good\n",
    "# size is .2 or .25 (percent) of your dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         4.00000000e+01, 6.37777778e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.70000000e+01, 6.70000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         2.70000000e+01, 4.80000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         3.87777778e+01, 5.20000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         4.80000000e+01, 7.90000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         3.80000000e+01, 6.10000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         4.40000000e+01, 7.20000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.50000000e+01, 5.80000000e+04]]),\n",
       " array([[1.0e+00, 0.0e+00, 1.0e+00, 0.0e+00, 3.0e+01, 5.4e+04],\n",
       "        [1.0e+00, 0.0e+00, 1.0e+00, 0.0e+00, 5.0e+01, 8.3e+04]]),\n",
       " array([1, 1, 1, 0, 1, 0, 0, 1]),\n",
       " array([0, 0]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling - Section 2-17. Using StandardScaler class. Note that most libraries do not require you to apply feature scaling (are taken care of for you).\n",
    "Looking at Age and Salary, the variables are not on the same scale. This will cause issues with your machine learning models because the Euclidean Distance between P1 and P2. Since Salary has a much wider range, the Euclidean Distance between Salary and Age will be will dominated by Salary (the square difference will be dominated by Salary values).\n",
    "\n",
    "That is why we need to use Feature Scaling on your data. Even if your model is not dependent on Euclidean Distance (e.g., Decision Trees model), we will still need to do Feature Scaling because the algorithm will converge much faster (or run slowly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Create an object of the StandardScaler class\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "# Then need to fit and transform on TRAIN set but only \n",
    "# transform on TEST set\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should you feature scale your dummy variables?\n",
    "Going to get a lot of debate on this. Depends on context, depends on how much interpretation you want to keep in your model. It won't break your model if you don't scale. But for now, we will scale those dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        ,  2.64575131, -0.77459667,  0.26306757,\n",
       "         0.12381479],\n",
       "       [-1.        ,  1.        , -0.37796447, -0.77459667, -0.25350148,\n",
       "         0.46175632],\n",
       "       [ 1.        , -1.        , -0.37796447,  1.29099445, -1.97539832,\n",
       "        -1.53093341],\n",
       "       [ 1.        , -1.        , -0.37796447,  1.29099445,  0.05261351,\n",
       "        -1.11141978],\n",
       "       [-1.        ,  1.        , -0.37796447, -0.77459667,  1.64058505,\n",
       "         1.7202972 ],\n",
       "       [ 1.        , -1.        , -0.37796447,  1.29099445, -0.0813118 ,\n",
       "        -0.16751412],\n",
       "       [-1.        ,  1.        , -0.37796447, -0.77459667,  0.95182631,\n",
       "         0.98614835],\n",
       "       [-1.        ,  1.        , -0.37796447, -0.77459667, -0.59788085,\n",
       "        -0.48214934]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        ,  2.64575131, -0.77459667, -1.45882927,\n",
       "        -0.90166297],\n",
       "       [ 1.        , -1.        ,  2.64575131, -0.77459667,  1.98496442,\n",
       "         2.13981082]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's important to .fit on X_train first before X_test, so that \n",
    "# they are both scaled on the same basis.\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to apply feature scaling to y (dependent variable)?\n",
    "# No, don't need to apply feature scaling. But if the dependent variable \n",
    "# takes a huge range of values, then you will need to feature scale it\n",
    "# But for this y, the value range is small since it's an encoded\n",
    "# categorical variable (yes or no; 1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Overview\n",
    "1. Get the dataset\n",
    "2. Importing the libraries\n",
    "3. Importing the Dataset\n",
    "4. Dealing with Missing Data\n",
    "5. Encoding Categorical Data\n",
    "6. Splitting the Dataset into the Training and Test sets\n",
    "7. Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.40000000e+01, 7.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        2.70000000e+01, 4.80000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        3.00000000e+01, 5.40000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        3.80000000e+01, 6.10000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        4.00000000e+01, 6.37777778e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.50000000e+01, 5.80000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        3.87777778e+01, 5.20000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.80000000e+01, 7.90000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e+01, 8.30000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.70000000e+01, 6.70000000e+04]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Template used for this class:\n",
    "1. Get the dataset\n",
    "2. Importing the libraries\n",
    "3. Importing the Dataset\n",
    "4. ~~Dealing with Missing Data~~\n",
    "5. ~~Encoding Categorical Data~~\n",
    "6. Splitting the Dataset into the Training and Test sets\n",
    "7. Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Importing the libraries\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n# Importing the dataset\\ndataset = pd.read_csv(\"Data.csv\")\\nX = dataset.iloc[:, :-1].values\\ny = datset.iloc[:, 3].values\\n\\n# Taking care of missing data\\nfrom sklearn.preprocessing import Imputer\\nimputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\\nimputer = imputer.fit(X[:, 1:3])\\nX[:, 1:3] = imputer.transform(X[:, 1:3])\\n\\n# Encoding categorical data\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\nlabelencoder_X = LabelEncoder()\\nX[:, 0] = labelencoder_X.fit_transform(X[:, 0])\\nonehotencoder = OneHotEncoder(categorical_features = [0])\\nX = onehotencoder.fit_transform(X).toarray()\\nlabelencoder_y = LabelEncoder()\\ny = labelencoder_y.fit_transform(y)\\n\\n# Splitting the dataset inot the Training and Test sets\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\\n\\n# Feature scaling\\nfrom sklearn.preprocessing import StandardScaler\\nsc_X = StandardScaler()\\nX_train = sc_X.fit_transform(X_train)\\nX_test = sc_X.transform(X_test)'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"Data.csv\")\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = datset.iloc[:, 3].values\n",
    "\n",
    "# Taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset inot the Training and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
